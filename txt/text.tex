\documentclass[a4paper,12pt,twoside]{ltxdoc}

%\usepackage[utf8]{inputenc} % not used, as I compile with Xelatex
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % Language, could be set to swedish
\usepackage{fancyhdr} % Header and footers
\usepackage{verbatim} % Better Verbatim packages
\usepackage{fancyvrb}
\usepackage{amssymb,amsmath}
\usepackage{graphicx} % PNG and SVG images, and such.
\usepackage{enumerate} % better listing options
\usepackage{hyperref} % URL and stuff.

\pagestyle{fancy}

\fancyhead[LH]{Group 11}
\fancyhead[RH]{Sentiment Analysis in Speech with Relation to Brands}
%\fancyfoot[LO]{\thepage}
%\fancyfoot[RE]{\thepage}
\title{Project: Search Engines and Information Retrieval Systems}
\author{
Jens Arvidsson <\href{mailto:jensarv@gmail.com}{jensarv@gmail.com}> \and
Fredrick Chahine <\href{mailto:fchahine@kth.se}{fchahine@kth.se}> \and
Erik Hallström  <\href{mailto:erik_hallstrom@icloud.com}{erik\_hallstrom@icloud.com}> \and
Petter Salminen <\href{mailto:petsal@kth.se}{petsal@kth.se}>}


% Kodkommando
\newcommand{\javafil}[1]{\lstinputlisting{#1}} % #1 = filnamn, #2 = caption
\newcommand{\ovning}[1]{\section*{Övning #1}}

\begin{document}
\maketitle
\tableofcontents

%\listoffigures
%\listoftables
\newpage
\begin{abstract}
Text goes here...

\end{abstract}

\newpage
\section{Background}
Today we find ourselves flooded with information. This information contains great value to companies, as they are
able to draw on consumer preferences and adjust their product lines accordingly. And yet, information may be difficult
to process for several reasons. One reason is the sheer amount of it. Another aspect of information that may make it difficult
to process is its format. The spoken word can be far more difficult to process that a written document. To capitalize on this treasure
grove of information, we need to develop effective ways of deriving details from the spoken word and processing them.

-------------------------

Technology is ever moving forward.

We're always looking for new ways to interact with your systems and devices. One of these ways to input information to computers would be by natural speech, which still is quite flimsy and hard for computers to analyse. 

With most of our media still being partly or fully in form of sound, such as web-radio, and television. It is hard to analyse all the information being run around on this media, as the format is not native to our computers and systems. 

We're always looking for new ways to interact with your systems and devices.
One of these ways to input information to computers would be by natural speech,
which still is quite flimsy and hard for computers to analyze. 

With most of our media still being partly or fully in form of sound, such  as web-radio, and television.
It is hard to analyze all the information being run around on this media, as the format is not
native to our computers and systems. 

With this is mind, our mission is to make an experimental program that can transcribe speech and analyze it, looking for certain trends.
For example, this can be used to recognize branding recognition from a audio stream, and then analyze the context to find an opinion about the brand.
A complete solution could be very interesting for companies who care to know how their brand is doing ``out there'' in the world.

To be able to find automatically information in the vast sea of digital audio/video media is a very challenging problem
because the information itself is very hard for computers to read. 



\section{Related Work}
\subsection{Speech-to-Text}
\emph{Speech-to-text}, or more generally \emph{Computer Speech Recognition} is the general problem in computer science where the computer has to translate spoken words into text.

The first real attempt to do speech recognition was made during the 50's and the system could only handle a small subset of speech, namely digit recognition. In 1952 reasearchers at Bell Laboratories built a speech recognition system that only could recognize digits by a single speaker in a quiet room. The thechnology of  the system was based on the theory of the acoustics of speech and the machine triggered on the sound of the vovels in the different digits.

Similar work was made by researchers at RCA Laboratories and at MIT Lincoln Lab during the 50's. The research teams at these two institutions built speech recognitions systems that could classify vovels that a test person spoke.

Moving on to the 60's, there were severeal Japanese teams that had progress with speech recognition, especially two professors named Suzuki and Nakata at Kyoto University. Even though they still only could recognize single digits or vovels, however the difference here was that these could be recognized in countinous speech. The earlier systems assumed each utterance of the test speaker contained the complete digit or vovel and thus did not need to be actively segmented.  

During the 70's the first speech recognition system that could handle spoken words an sentences began to be developed. A program named "Harpy" was made at Carnegie Mellon University and could recognize speech containing 1011 different words. It had a moderate preformance . Harpy used a graph search algorithm to obtain the results. 

A new technology based on the n-gram language model, was introduced in the beginning of the 80's and have been very important to speech recognition systems ever since. One obvious task for a speech recognizer is to use it as a automatic typewriter; a user speaks words and sentences that should be written to a document. Since there is an underlying structure and grammar in all form of language, not all letter orderings and word orderings are equally probable. In fact, some orderings are even wrong, i.e the words are misspelled or sentences are gramatically incorrect. The n-gram model looks at a sequence of words and tells how probable it is. That will help when doing the word recognition, we will get a prior for each word and thus use a M.A.P estimation for what the next word ought to be.

These typewriters was suppose to be used extensively by certain persons, it thus allowed for individual training of the system. This was not the case for another application of speech recognition, namely routing phonecalls

Today the use of Speech-to-Text can be found in many different fields and applications.

As an example, court reporting where everything said in court could be recorded and automatically transcribed by a computer (replacing human labor).
Hands-free computing, meaning one could write messages and emails by just talking to the computer.
Automatic calling exchanges at service numbers,
such that one could ask the problem and get directed to a expert within the specific field.

Or on current generation of cellphones as popularized by the Apple iPhone's application Siri
- supplying an intelligent personal assistant and knowledge navigator that you can ask questions in natural language and get an answer. 

%% Text about some how speech-to-text works.

\subsection{Sentiment Analysis}
The \emph{Sentiment Analysis} or \emph{Opinion Mining} problem is that given any snippet of text,
to find the general attitude of given text on a topic.
As an very simple example, the text ``I hate Mondays'' has a very negative attitude,
while the attitude of ``I would not prefer Mondays'' is more towards neutral.

Some early work in the area would be for example to sample reviews of a movie, or restaurant and analyze the polarity of it. Early work on this has been acknowledged to be done by Turney\cite{turney} and Pang\cite{pang}

To be able to get the general opinion or attitude from a text can be very useful for companies that care about their reputation.
Thus for example finding a lot of complains about something could give great automatic feedback.

With extreme rise of social media platforms such as \emph{Twitter} and \emph{Facebook}, the general interest in sentiment analysis has skyrocketed. Businesses look for solutions to automate the data mining of these platforms, analyzing the general opinion and if case getting negative feedback, to be able to immediately attend to the issues.

\section{Method}

Our solutin to this problem consists of three steps, which are later combined into a single program.

1. Our first step is to work convert speech to text. For this we use Google's speech API.

2. Next, we process the converted text to capture brands that the data miner is interested in.

3. Finally, we process the expressions related to these brands to detect sentiments, using Gavagai's API.

Google's speech API is used widely across its product range, from web search to translate to Android voice commands.
Provided with an audio file in FLAC format, the API returns a JSON object representing the transcribed audio. The JSON object
can then be parsed to retrieve the text string that corresponds to the spoken message. Google's speech API allows for a number
of different configuration settings, including language and audio frequency of the supplied file.

In our project, audio file input is extended by letting the user of the program record an audio snippet in real time and
then submit it for processing.

In the second step, mentions of brands are elicited from the transcribed audio. In our program, the user supplies the name
of a brand that he or she wants to watch for. The user also chooses how many words before an after the brand name should
be included in the analysis. Thus, there is a precision-vs-recall tradeoff here. Choosing a large number of words to include
in the processing increase the scope of the search, but might lower the precision as unrelated adjectives may be grouped
with the brand mention.

In the third step, we provide cardinal scores of the audio according to a number of criteria. Gavagai's API lets us
evaluate a text string based on a number of parameters. We can look for the sentiments described as positive, negative, sexy, violent,
and uncertain. The program then presents the user with the data tabulated as shown in the figure. %% bild här

--------------------

Our solution to this problem consists not of trying to reinvent the wheel, but of employing different
solutions that are more-or-less complete throughout the different stages, and then stitching them together.

First and foremost we have an audio file, which we will send to Google speech API, which will give us a fair caption of the given sound segment.

This text we can then search for given brands or other point of interests, and then send this text for textual attitude analyzing using an web API provided by our mentor at Gavagai.

From here we will get an score, of which we can show the user what kind of attitude the speech had. 

\section{Results}


\section{Discussion}


\newpage 

%% Here is the ref system, where you add an item
%% as you can see with the \bibitem{NAME} field.
%%
%% To in the text refer to a certain reference, use
%% this command \cite{NAME}
%%
\begin{thebibliography}{9}

\bibitem{example}
  Leslie Lamport,
  \emph{\LaTeX: A Document Preparation System}.
  Addison Wesley, Massachusetts,
  2nd Edition,
  1994.

%% This thing seems to be THE SHIT to cite, has 17k of them on google scholar.
\bibitem{speech}
Rabiner, Lawrence R.
\emph{A tutorial on hidden Markov models and selected applications in speech recognition.}
Proceedings of the IEEE 77.2,
pp. 257-286
1989.

\bibitem{turney}
Peter Turney, 
\emph{Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews},
 Proceedings of the Association for Computational Linguistics,
 pp. 417-424
 2002.

\bibitem{pang}
  Bo Pang; Lillian Lee and Shivakumar Vaithyanathan,
  \emph{Thumbs up? Sentiment Classification using Machine Learning Techniques},
  Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).
  pp. 79-86,
  2002.

\bibitem{attitude}
  Shanahan, James G., Yan Qu, and Janyce Wiebe, eds.
  \emph{Computing attitude and affect in text: theory and applications.}
  Springer, 
  Vol. 20.
  2006.

\end{thebibliography}

\end{document}
